{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32f8ca24",
   "metadata": {},
   "source": [
    "# Understanding Hired Rides in NYC\n",
    "\n",
    "_[Project prompt](https://docs.google.com/document/d/1uAUJGEUzfNj6OsWNAimnYCw7eKaHhMUfU1MTj9YwYw4/edit?usp=sharing), [grading rubric](https://docs.google.com/document/d/1hKuRWqFcIdhOkow3Nljcm7PXzIkoa9c_aHkMKZDxWa0/edit?usp=sharing)_\n",
    "\n",
    "_This scaffolding notebook may be used to help setup your final project. It's **totally optional** whether you make use of this or not._\n",
    "\n",
    "_If you do use this notebook, everything provided is optional as well - you may remove or add prose and code as you wish._\n",
    "\n",
    "_**All code below should be consider \"pseudo-code\" - not functional by itself, and only an outline to help you with your own approach.**_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f75fd94",
   "metadata": {},
   "source": [
    "## Project Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66dcde05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all import statements needed for the project, for example:\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import bs4\n",
    "import sqlite3\n",
    "import matplotlib.pyplot as plt\n",
    "import sqlalchemy as db\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "import wget\n",
    "import geopandas as gpd\n",
    "import xorbits\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3f1242c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# any constants you might need; some have been added for you, and \n",
    "# some you need to fill in\n",
    "\n",
    "TAXI_URL = \"https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "\n",
    "TAXI_ZONES_DIR = \"data/taxi_zones\"\n",
    "\n",
    "TAXI_ZONES_SHAPEFILE = \"taxi_zones.shp\"\n",
    "\n",
    "\n",
    "CRS = 4326  # coordinate reference system\n",
    "\n",
    "# (lat, lon)\n",
    "NEW_YORK_BOX_COORDS = ((40.560445, -74.242330), (40.908524, -73.717047))\n",
    "LGA_BOX_COORDS = ((40.763589, -73.891745), (40.778865, -73.854838))\n",
    "JFK_BOX_COORDS = ((40.639263, -73.795642), (40.651376, -73.766264))\n",
    "EWR_BOX_COORDS = ((40.686794, -74.194028), (40.699680, -74.165205))\n",
    "\n",
    "DATABASE_URL = \"sqlite://project.db\"\n",
    "DATABASE_SCHEMA_FILE = \"schema.sql\"\n",
    "QUERY_DIRECTORY = \"queries\"\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "uber_path = \"/Users/xuzh/Desktop/TFA/uber_rides_sample.csv\"\n",
    "weather_path1 = \"/Users/xuzh/Desktop/TFA/\"\n",
    "yellowtaxi_path = \"/Users/xuzh/Desktop/TFA/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d6601633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the QUERY_DIRECTORY exists\n",
    "try:\n",
    "    os.mkdir(QUERY_DIRECTORY)\n",
    "except Exception as e:\n",
    "    if e.errno == 17:\n",
    "        # the directory already exists\n",
    "        pass\n",
    "    else:\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ad10ea",
   "metadata": {},
   "source": [
    "## Part 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d53e24",
   "metadata": {},
   "source": [
    "### Load Taxi Zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "58708809",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the taxi shp file\n",
    "def load_taxi_zones(shapefile):\n",
    "    s=gpd.read_file(shapefile)\n",
    "    return s\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8d04c726",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if the input location id is within the required zones\n",
    "def lookup_coords_for_taxi_zone_id( loaded_taxi_zones,zone_loc_id=None):\n",
    "    #print(loaded_taxi_zones)\n",
    "    s=load_taxi_zones(TAXI_ZONES_SHAPEFILE)\n",
    "    df=pd.DataFrame()\n",
    "    df['ID']=s['OBJECTID']\n",
    "    #print(df[\"ID\"])\n",
    "    if loaded_taxi_zones in df['ID'].unique():\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "68b18f59",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "#test cell\n",
    "print(lookup_coords_for_taxi_zone_id(299))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93daa717",
   "metadata": {},
   "source": [
    "### Process Taxi Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1dd682b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_urls_from_taxi_page(taxi_page):\n",
    "    # send a request to the taxi page and get its HTML content\n",
    "    response = requests.get(taxi_page)\n",
    "    content = response.content\n",
    "    \n",
    "    # create a BeautifulSoup object and find all links on the page\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "    \n",
    "    links = soup.find_all('a')\n",
    "    \n",
    "    # extract URLs from the links and store them in a list\n",
    "    urls = []\n",
    "    for link in links:\n",
    "        url = link.get('href')\n",
    "        if url and url.endswith('.parquet') and 'yellow_tripdata' in url:\n",
    "            urls.append(url)\n",
    "\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1f331527",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test cell: get_all_urls_from_taxi_page will not return a empty value\n",
    "assert get_all_urls_from_taxi_page(TAXI_URL) != None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd0d198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_taxi_parquet_urls(all_urls):\n",
    "    #down data from years 2008 to 2015\n",
    "    all_urls = get_all_urls_from_taxi_page(TAXI_URL)\n",
    "    l=list(range(2009,2016))\n",
    "    l=[str(x) for x in l]\n",
    "    urls_fillter=[]\n",
    "    for url in all_urls:\n",
    "        for flag in l:\n",
    "            if (flag in url):\n",
    "                urls_fillter.append(url)\n",
    "    print(urls_fillter)\n",
    "    return urls_fillter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "35c9c0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_taxi_data(parquet_urls):\n",
    "    all_taxi_dataframes = []\n",
    "    \n",
    "    for parquet_url in parquet_urls:\n",
    "        # maybe: first try to see if you've downloaded this exact\n",
    "        # file already and saved it before trying again\n",
    "        dataframe = get_and_clean_month(parquet_url)\n",
    "        add_distance_column(dataframe)\n",
    "        # maybe: if the file hasn't been saved, save it so you can\n",
    "        # avoid re-downloading it if you re-run the function\n",
    "        \n",
    "        all_taxi_dataframes.append(dataframe)\n",
    "        \n",
    "    # create one gigantic dataframe with data from every month needed\n",
    "    taxi_data = pd.contact(all_taxi_dataframes)\n",
    "    return taxi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "200776ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_taxi_data():\n",
    "    #download the required data\n",
    "    all_urls = filter_taxi_parquet_urls(TAXI_URL)\n",
    "    for url in all_urls:\n",
    "        wget.download(url,\"/Users/xuzh/Desktop/TFA/queries\"+url.split('/')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a7f28900",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-8620e8173b48>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_taxi_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-50-49dcfc1f705c>\u001b[0m in \u001b[0;36mget_taxi_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_taxi_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m#download the required data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mall_urls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilter_taxi_parquet_urls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTAXI_URL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0murl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_urls\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;31m#print(r\"C:\\py-work\\work23\\projects\\c0428_2\\queries\\\\\"+url.split('/')[-1])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-46-512fc0882cb6>\u001b[0m in \u001b[0;36mfilter_taxi_parquet_urls\u001b[0;34m(all_urls)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfilter_taxi_parquet_urls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_urls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m#down data from years 2008 to 2015\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mall_urls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_all_urls_from_taxi_page\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTAXI_URL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0ml\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2009\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2016\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0ml\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-44-d4ad213cd8fb>\u001b[0m in \u001b[0;36mget_all_urls_from_taxi_page\u001b[0;34m(taxi_page)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_all_urls_from_taxi_page\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtaxi_page\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# send a request to the taxi page and get its HTML content\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtaxi_page\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'allow_redirects'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'get'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    530\u001b[0m         \u001b[0mproxies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproxies\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m         settings = self.merge_environment_settings(\n\u001b[0m\u001b[1;32m    533\u001b[0m             \u001b[0mprep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcert\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m         )\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mmerge_environment_settings\u001b[0;34m(self, url, proxies, stream, verify, cert)\u001b[0m\n\u001b[1;32m    709\u001b[0m             \u001b[0;31m# Set environment's proxies.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             \u001b[0mno_proxy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'no_proxy'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mproxies\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 711\u001b[0;31m             \u001b[0menv_proxies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_environ_proxies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_proxy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mno_proxy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    712\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menv_proxies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m                 \u001b[0mproxies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/requests/utils.py\u001b[0m in \u001b[0;36mget_environ_proxies\u001b[0;34m(url, no_proxy)\u001b[0m\n\u001b[1;32m    774\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m     \"\"\"\n\u001b[0;32m--> 776\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mshould_bypass_proxies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_proxy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mno_proxy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    777\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/requests/utils.py\u001b[0m in \u001b[0;36mshould_bypass_proxies\u001b[0;34m(url, no_proxy)\u001b[0m\n\u001b[1;32m    758\u001b[0m         \u001b[0;31m# parsed.hostname can be `None` in cases such as a file URI.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 760\u001b[0;31m             \u001b[0mbypass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproxy_bypass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhostname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    761\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgaierror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m             \u001b[0mbypass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/urllib/request.py\u001b[0m in \u001b[0;36mproxy_bypass\u001b[0;34m(host)\u001b[0m\n\u001b[1;32m   2653\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mproxy_bypass_environment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2654\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2655\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mproxy_bypass_macosx_sysconf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2657\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgetproxies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/urllib/request.py\u001b[0m in \u001b[0;36mproxy_bypass_macosx_sysconf\u001b[0;34m(host)\u001b[0m\n\u001b[1;32m   2630\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mproxy_bypass_macosx_sysconf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2631\u001b[0m         \u001b[0mproxy_settings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_proxy_settings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2632\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_proxy_bypass_macosx_sysconf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxy_settings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2634\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgetproxies_macosx_sysconf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/urllib/request.py\u001b[0m in \u001b[0;36m_proxy_bypass_macosx_sysconf\u001b[0;34m(host, proxy_settings)\u001b[0m\n\u001b[1;32m   2598\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhostIP\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2599\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2600\u001b[0;31m                     \u001b[0mhostIP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgethostbyname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhostonly\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2601\u001b[0m                     \u001b[0mhostIP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mip2num\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhostIP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2602\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#This step may take very long, depending on your download speed\n",
    "get_taxi_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "49ca055d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#clean and filter yellow taxi data from 2009\n",
    "def get_yellowtaxi_9(path):\n",
    "    df = pd.read_parquet(path, engine='pyarrow')\n",
    "    new_df=df.drop(['vendor_name', 'Rate_Code',\n",
    " 'store_and_forward', 'Payment_Type' ,'Fare_Amt',\n",
    " 'surcharge' ,'mta_tax','Tolls_Amt' ,'Total_Amt'], axis=1)\n",
    "    modi_df = new_df[(40.560445 < new_df['Start_Lat']) & (new_df['Start_Lat'] < 40.908524) &\n",
    "                     (40.560445 < new_df['End_Lat']) & (new_df['End_Lat'] < 40.908524) &\n",
    "                     (-73.717047 > new_df['End_Lon']) & (new_df['End_Lon'] > -74.242330) &\n",
    "                     (-73.717047 > new_df['Start_Lon']) & (new_df['Start_Lon'] > -74.242330)]\n",
    "\n",
    "\n",
    "    df=modi_df.dropna(axis=0, how='any', inplace=False)\n",
    "    df = df.drop(['Start_Lon','Start_Lat','End_Lon','End_Lat'], axis=1)\n",
    "    df=df.sample(n=2380,random_state=1)\n",
    "    df[\"Trip_Pickup_Date\"]=pd.to_datetime(df['Trip_Pickup_DateTime']).dt.date\n",
    "    df['Trip_Pickup_DateTime'] = pd.to_datetime(df['Trip_Pickup_DateTime'])\n",
    "    df['Trip_Pickup_DateHour'] = df['Trip_Pickup_DateTime'].dt.strftime('%H')\n",
    "    df=df.drop(['Trip_Pickup_DateTime', 'Trip_Dropoff_DateTime',], axis=1)\n",
    "    df.columns = ['passenger_count', 'trip_distance', 'tip_amount','Trip_Pickup_Date', 'Trip_Pickup_DateHour']\n",
    "    # print(df.dtypes)\n",
    "    # print(df.head())\n",
    "    return df\n",
    "#clean and filter yellow taxi data from 2012\n",
    "def get_yellowtaxi_12(path):\n",
    "    df = pd.read_parquet(path, engine='pyarrow')\n",
    "    # df=df.sample(n=3000,random_state=1)\n",
    "    new_df = df.drop(['tpep_dropoff_datetime', 'RatecodeID','VendorID',\n",
    "                      'store_and_fwd_flag', 'payment_type', 'fare_amount',\n",
    "                      'extra', 'tolls_amount', 'improvement_surcharge', 'total_amount', 'congestion_surcharge','mta_tax',\n",
    "                      'airport_fee'], axis=1)\n",
    "    # print(new_df.head())\n",
    "    # new_df[\"pul\"] = new_df['PULocationID'].apply(lookup_coords_for_taxi_zone_id)\n",
    "    # new_df[\"dol\"] = new_df['DOLocationID'].apply(lookup_coords_for_taxi_zone_id)\n",
    "    modi_df = new_df[(new_df['PULocationID'] <= 263) & (new_df['DOLocationID'] <= 263)]\n",
    "    # print(new_df.head())\n",
    "    df = modi_df.dropna(axis=0, how='any', inplace=False)\n",
    "    df = df.drop(['PULocationID', 'DOLocationID', ], axis=1)\n",
    "    df[\"Trip_Pickup_Date\"] = pd.to_datetime(df['tpep_pickup_datetime']).dt.date\n",
    "    df['Trip_Pickup_DateTime'] = pd.to_datetime(df['tpep_pickup_datetime'])\n",
    "    df['Trip_Pickup_DateHour'] = df['tpep_pickup_datetime'].dt.strftime('%d')\n",
    "    df=df[(df['Trip_Pickup_DateHour'])]\n",
    "    df = df.drop(['Trip_Pickup_DateTime'], axis=1)\n",
    "    df = df.drop(['tpep_pickup_datetime'], axis=1)\n",
    "    #print(df.head())\n",
    "    df.columns = ['passenger_count', 'trip_distance', 'tip_amount','Trip_Pickup_Date', 'Trip_Pickup_DateHour']\n",
    "    return df\n",
    "#clean and filter yellow taxi data from 2015\n",
    "def get_yellowtaxi_15(path):\n",
    "    df = pd.read_parquet(path, engine='pyarrow')\n",
    "    # df=df.sample(n=3000,random_state=1)\n",
    "    new_df = df.drop(['tpep_dropoff_datetime', 'RatecodeID','VendorID',\n",
    "                      'store_and_fwd_flag', 'payment_type', 'fare_amount',\n",
    "                      'extra', 'tolls_amount', 'improvement_surcharge', 'total_amount', 'congestion_surcharge','mta_tax','trip_distance', 'tip_amount',\n",
    "                      'airport_fee'], axis=1)\n",
    "   # print(new_df.head())\n",
    "    # new_df[\"pul\"] = new_df['PULocationID'].apply(lookup_coords_for_taxi_zone_id)\n",
    "    # new_df[\"dol\"] = new_df['DOLocationID'].apply(lookup_coords_for_taxi_zone_id)\n",
    "    modi_df = new_df[(new_df['PULocationID'] <= 263) & (new_df['DOLocationID'] <= 263)]\n",
    "    # print(new_df.head())\n",
    "    df = df.sample(n=2380, random_state=1)\n",
    "    df = modi_df.dropna(axis=0, how='any', inplace=False)\n",
    "    #df = df.drop(['PULocationID', 'DOLocationID', ], axis=1)\n",
    "    # df[\"Trip_Pickup_Date\"] = pd.to_datetime(df['tpep_pickup_datetime']).dt.date\n",
    "    # df = df.drop(['Trip_Pickup_DateTime'], axis=1)\n",
    "    df = df.drop(['PULocationID'], axis=1)\n",
    "    #df = df.drop(['tpep_pickup_datetime'], axis=1)\n",
    "    #print(df.head())\n",
    "    #df.columns = ['passenger_count',  'Trip_Pickup_DateHour']\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4a17b788",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#clean and filter yellow taxi data from 2010\n",
    "\n",
    "def get_yellowtaxi_10(path):\n",
    "    df = pd.read_parquet(path, engine='pyarrow')\n",
    "    #print(df.head())\n",
    "    new_df=df.drop(['vendor_id', 'rate_code',\n",
    " 'store_and_fwd_flag', 'payment_type' ,'fare_amount',\n",
    " 'surcharge' ,'mta_tax','tolls_amount' ,'total_amount'], axis=1)\n",
    "    modi_df = new_df[(40.560445 < new_df['pickup_latitude']) & (new_df['pickup_latitude'] < 40.908524) &\n",
    "                     (40.560445 < new_df['dropoff_latitude']) & (new_df['dropoff_latitude'] < 40.908524) &\n",
    "                     (-73.717047 > new_df['dropoff_longitude']) & (new_df['dropoff_longitude'] > -74.242330) &\n",
    "                     (-73.717047 > new_df['pickup_longitude']) & (new_df['pickup_longitude'] > -74.242330)]\n",
    "\n",
    "\n",
    "    df=modi_df.dropna(axis=0, how='any', inplace=False)\n",
    "    df = df.drop(['pickup_latitude','dropoff_latitude','dropoff_longitude','pickup_longitude'], axis=1)\n",
    "    df=df.sample(n=2380,random_state=1)\n",
    "    df[\"Trip_Pickup_Date\"]=pd.to_datetime(df['pickup_datetime']).dt.date\n",
    "    df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'])\n",
    "    df['Trip_Pickup_DateHour'] = df['pickup_datetime'].dt.strftime('%H')\n",
    "    df=df.drop(['pickup_datetime', 'dropoff_datetime',], axis=1)\n",
    "    #print(df.head())\n",
    "    df.columns = ['passenger_count', 'trip_distance', 'tip_amount','Trip_Pickup_Date', 'Trip_Pickup_DateHour']\n",
    "    # print(df.dtypes)\n",
    "    # print(df.head())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6b4a7f5f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#clean and filter yellow taxi data from 2011\n",
    "def get_yellowtaxi_11(path):\n",
    "    df = pd.read_parquet(path, engine='pyarrow')\n",
    "    # df=df.sample(n=3000,random_state=1)\n",
    "    new_df = df.drop(['tpep_dropoff_datetime', 'RatecodeID','VendorID',\n",
    "                      'store_and_fwd_flag', 'payment_type', 'fare_amount',\n",
    "                      'extra', 'tolls_amount', 'improvement_surcharge', 'total_amount', 'congestion_surcharge','mta_tax',\n",
    "                      'airport_fee'], axis=1)\n",
    "    # print(new_df.head())\n",
    "    # new_df[\"pul\"] = new_df['PULocationID'].apply(lookup_coords_for_taxi_zone_id)\n",
    "    # new_df[\"dol\"] = new_df['DOLocationID'].apply(lookup_coords_for_taxi_zone_id)\n",
    "    modi_df = new_df[(new_df['PULocationID'] <= 263) & (new_df['DOLocationID'] <= 263)]\n",
    "    # print(new_df.head())\n",
    "    df = modi_df.dropna(axis=0, how='any', inplace=False)\n",
    "    df = df.drop(['PULocationID', 'DOLocationID', ], axis=1)\n",
    "    df = df.sample(n=2380, random_state=1)\n",
    "    df[\"Trip_Pickup_Date\"] = pd.to_datetime(df['tpep_pickup_datetime']).dt.date\n",
    "    df['Trip_Pickup_DateTime'] = pd.to_datetime(df['tpep_pickup_datetime'])\n",
    "    df['Trip_Pickup_DateHour'] = df['tpep_pickup_datetime'].dt.strftime('%H')\n",
    "    df = df.drop(['Trip_Pickup_DateTime'], axis=1)\n",
    "    df = df.drop(['tpep_pickup_datetime'], axis=1)\n",
    "    #print(df.head())\n",
    "    df.columns = ['passenger_count', 'trip_distance', 'tip_amount','Trip_Pickup_Date', 'Trip_Pickup_DateHour']\n",
    "    # print(df.dtypes)\n",
    "    # print(df.head())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1f073cc5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_add_yellowtaxi():\n",
    "    # urls=[r'C:\\py-work\\work23\\projects\\c0428_2\\\\queriesyellow_tripdata_2015-05.parquet',\n",
    "    #       r'C:\\py-work\\work23\\projects\\c0428_2\\\\queriesyellow_tripdata_2009-05.parquet',\n",
    "    #       r'C:\\py-work\\work23\\projects\\c0428_2\\\\queriesyellow_tripdata_2013-05.parquet',\n",
    "    #       r'C:\\py-work\\work23\\projects\\c0428_2\\\\queriesyellow_tripdata_2015-01.parquet',]\n",
    "    urls=filter_taxi_parquet_urls(get_all_urls_from_taxi_page(TAXI_URL))\n",
    "    df=[]\n",
    "    # df.columns = ['passenger_count', 'trip_distance', 'tip_amount','Trip_Pickup_Date', 'Trip_Pickup_DateHour']\n",
    "    # pd.concat([df,])\n",
    "    for url in urls:\n",
    "        filename=yellowtaxi_path+'queries'+url.split('/')[-1]\n",
    "        print(filename)\n",
    "        if '2009' in filename:\n",
    "            write_yellow(get_yellowtaxi_9(filename))\n",
    "            #df.append(get_yellowtaxi_9(filename))\n",
    "        elif '2010' in filename:\n",
    "            write_yellow(get_yellowtaxi_10(filename))\n",
    "            #df.append(get_yellowtaxi_10(filename))\n",
    "        else:\n",
    "            write_yellow(get_yellowtaxi_11(filename))\n",
    "            #df.append(get_yellowtaxi_11(filename))\n",
    "            # pd.concat([df,get_yellowtaxi_11(filename)],axis=0)\n",
    "    #pd.concat(df,axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc94659",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "094b4d6d",
   "metadata": {},
   "source": [
    "### Processing Uber Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7c58e3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uberdata(path):\n",
    "    df = pd.read_csv(path, low_memory=False)\n",
    "    new_df = df.drop(['Unnamed: 0', 'fare_amount', 'key'], axis=1)\n",
    "    modi_df = new_df[(40.560445 < new_df['pickup_latitude']) & (new_df['pickup_latitude'] < 40.908524) &\n",
    "                     (40.560445 < new_df['dropoff_latitude']) & (new_df['dropoff_latitude'] < 40.908524) &\n",
    "                     (-73.717047 > new_df['dropoff_longitude']) & (new_df['dropoff_longitude'] > -74.242330) &\n",
    "                     (-73.717047 > new_df['pickup_longitude']) & (new_df['pickup_longitude'] > -74.242330)]\n",
    "    modi_df = modi_df.reset_index(drop=True)\n",
    "    modi_df['pickup_datetime']=pd.to_datetime(modi_df['pickup_datetime'])\n",
    "    # modi_df['pickup_datetime']=modi_df['pickup_datetime'].dt.strftime('%Y-%m-%d %H')\n",
    "\n",
    "    modi_df=modi_df.drop(['pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude'],axis=1)\n",
    "    return modi_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "de3b5ad9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            pickup_datetime  passenger_count\n",
      "0 2015-05-07 19:52:06+00:00                1\n",
      "1 2009-07-17 20:04:56+00:00                1\n",
      "2 2009-08-24 21:45:00+00:00                1\n",
      "3 2009-06-26 08:22:21+00:00                3\n",
      "4 2014-08-28 17:47:00+00:00                5\n"
     ]
    }
   ],
   "source": [
    "uber_df=get_uberdata(uber_path)\n",
    "print(uber_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a15cbb",
   "metadata": {},
   "source": [
    "### Processing Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "76e864ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read a local weather data csv file, normalize column names, and put into a dataframe\n",
    "def get_hour_weather(path):\n",
    "    df = pd.read_csv(path, low_memory=False)\n",
    "    new_df = df[[\"DATE\", \"REPORT_TYPE\", \"HourlyWindSpeed\", \"HourlyPrecipitation\"]]\n",
    "\n",
    "    new_df['DATE']=pd.to_datetime(new_df['DATE'])\n",
    "    new_df['nDATE']=new_df['DATE'].dt.strftime('%M')\n",
    "    new_df['ndate']=new_df['DATE']\n",
    "    modi_df = new_df[(new_df['nDATE']=='51')]\n",
    "    modi_df = modi_df.reset_index(drop=True)\n",
    "    modi_df = modi_df.drop(['REPORT_TYPE','nDATE'], axis=1)\n",
    "    #print(modi_df.head())\n",
    "    return modi_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b0a988c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test cell\n",
    "#get_hour_weather('/Users/xuzh/Desktop/TFA/2009_weather.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0687581f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize data in Hourly Precipitation\n",
    "def get_day_weather(weather):\n",
    "    weather['DATE'] = pd.to_datetime(weather['DATE']).dt.date\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace(np.nan, 0)\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace(\"T\", 0)\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace(\"0.01s\", 0.01)\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace(\"0.02s\", 0.02)\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace(\"0.03s\", 0.03)\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace(\"0.04s\", 0.04)\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace(\"0.05s\", 0.05)\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace(\"0.06s\", 0.06)\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace(\"0.07s\", 0.07)\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace(\"0.08s\", 0.08)\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace(\"0.09s\", 0.09)\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace(\"0.10s\", 0.1)\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace(\"0.11s\", 0.11)\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace(\"0.12s\", 0.12)\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace(\"0.13s\", 0.13)\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace(\"0.14s\", 0.14)\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace(\"0.15s\", 0.15)\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace(\"0.16s\", 0.16)\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace(\"0.17s\", 0.17)\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace(\"0.18s\", 0.18)\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace(\"0.19s\", 0.19)\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace(\"0.20s\", 0.2)\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace(\"0.21s\", 0.21)\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace(\"0.22s\", 0.22)\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace(\"0.23s\", 0.23)\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace(\"0.24s\", 0.24)\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace(\"0.25s\", 0.25)\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace(\"0.26s\", 0.26)\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace(\"0.27s\", 0.27)\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace(\"0.28s\", 0.28)\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace(\"0.29s\", 0.29)\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace(\"0.30s\", 0.3)\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace(\"0.31s\", 0.31)\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace(\"0.32s\", 0.32)\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace(\"0.33s\", 0.33)\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace(\"0.34s\", 0.34)\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace(\"0.35s\", 0.35)\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace(\"0.36s\", 0.36)\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace(\"0.37s\", 0.37)\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace(\"0.38s\", 0.38)\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace(\"0.39s\", 0.39)\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace(\"0.40s\", 0.4)\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace(\"0.41s\", 0.41)\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace(\"0.42s\", 0.42)\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace(\"0.43s\", 0.43)\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace(\"0.44s\", 0.44)\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace(\"0.45s\", 0.45)\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace(\"0.46s\", 0.46)\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace(\"0.47s\", 0.47)\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace(\"0.48s\", 0.48)\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace(\"0.49s\", 0.49)\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace(\"0.50s\", 0.5)\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace(\"0.51s\", 0.51)\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace(\"0.52s\", 0.52)\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace(\"0.53s\", 0.53)\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace(\"0.54s\", 0.54)\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace(\"0.55s\", 0.55)\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace(\"0.56s\", 0.56)\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace(\"0.57s\", 0.57)\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace(\"0.58s\", 0.58)\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace(\"0.59s\", 0.59)\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace(\"0.60s\", 0.6)\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace(\"0.61s\", 0.61)\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace(\"0.62s\", 0.62)\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace(\"0.63s\", 0.63)\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace(\"0.64s\", 0.64)\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace(\"0.65s\", 0.65)\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace(\"0.66s\", 0.66)\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace(\"0.67s\", 0.67)\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace(\"0.68s\", 0.68)\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace(\"0.69s\", 0.69)\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace(\"0.70s\", 0.7)\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace(\"0.71s\", 0.71)\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace(\"0.72s\", 0.72)\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace(\"0.73s\", 0.73)\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace(\"0.74s\", 0.74)\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace(\"0.75s\", 0.75)\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace(\"0.76s\", 0.76)\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace(\"0.77s\", 0.77)\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace(\"0.78s\", 0.78)\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace(\"0.79s\", 0.79)\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace(\"0.80s\", 0.8)\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace(\"0.81s\", 0.81)\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace(\"0.82s\", 0.82)\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace(\"0.83s\", 0.83)\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace(\"0.84s\", 0.84)\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace(\"0.85s\", 0.85)\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace(\"0.86s\", 0.86)\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace(\"0.87s\", 0.87)\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace(\"0.88s\", 0.88)\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace(\"0.89s\", 0.89)\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace(\"0.90s\", 0.9)\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace(\"0.91s\", 0.91)\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace(\"0.92s\", 0.92)\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace(\"0.93s\", 0.93)\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace(\"0.94s\", 0.94)\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace(\"0.95s\", 0.95)\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace(\"0.96s\", 0.96)\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace(\"0.97s\", 0.97)\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace(\"0.98s\", 0.98)\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace(\"0.99s\", 0.99)\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].astype(float)\n",
    "    #print(weather)\n",
    "    weather_day1 = weather.groupby('DATE')['HourlyWindSpeed'].mean()\n",
    "    weather_day2 = weather.groupby('DATE')['HourlyPrecipitation'].sum()\n",
    "    weather_day1 = weather_day1.to_frame()\n",
    "    weather_day2 = weather_day2.to_frame()\n",
    "    weather_day1['HourlyPrecipitation']=weather_day2['HourlyPrecipitation']\n",
    "    #print(weather_day1)\n",
    "    return weather_day1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3ef8945d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load and clean weather data from 2009 to 2015\n",
    "def load_and_clean_weather_data(file_path):\n",
    "    weather0 = get_hour_weather(file_path+'2009_weather.csv')\n",
    "    a0 = get_day_weather(weather0)\n",
    "    l=list(range(2009,2016))\n",
    "    l=[str(x) for x in l]\n",
    "    for i in l:\n",
    "        file_name=file_path+f'{i}_weather.csv'\n",
    "        print(file_name)\n",
    "        weather1 = get_hour_weather(file_name)\n",
    "        #print(weather1)\n",
    "        a = get_day_weather(weather1)\n",
    "        #print(a)\n",
    "        #write_hourlyweather(weather1)\n",
    "        # write_daylyweather(a)\n",
    "\n",
    "\n",
    "        pd.concat([weather0,weather1],axis=0)\n",
    "        pd.concat([a0,a],axis=0)\n",
    "\n",
    "    # create two dataframes with hourly & daily data from every month\n",
    "    hourly_data = weather0\n",
    "    daily_data = a0\n",
    "\n",
    "    return hourly_data, daily_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ee00a4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test cell\n",
    "#load_and_clean_weather_data(weather_path1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f7cd53a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-23-1551ad0c6435>:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df['DATE']=pd.to_datetime(new_df['DATE'])\n",
      "<ipython-input-23-1551ad0c6435>:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df['nDATE']=new_df['DATE'].dt.strftime('%M')\n",
      "<ipython-input-23-1551ad0c6435>:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df['ndate']=new_df['DATE']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/guowenlu/Desktop/TFA/2009_weather.csv\n",
      "/Users/guowenlu/Desktop/TFA/2010_weather.csv\n",
      "/Users/guowenlu/Desktop/TFA/2011_weather.csv\n",
      "/Users/guowenlu/Desktop/TFA/2012_weather.csv\n",
      "/Users/guowenlu/Desktop/TFA/2013_weather.csv\n",
      "/Users/guowenlu/Desktop/TFA/2014_weather.csv\n",
      "/Users/guowenlu/Desktop/TFA/2015_weather.csv\n",
      "         DATE  HourlyWindSpeed  HourlyPrecipitation               ndate\n",
      "0  2009-01-01             18.0                  0.0 2009-01-01 00:51:00\n",
      "1  2009-01-01             18.0                  0.0 2009-01-01 01:51:00\n",
      "2  2009-01-01             18.0                  0.0 2009-01-01 02:51:00\n",
      "3  2009-01-01              8.0                  0.0 2009-01-01 03:51:00\n",
      "4  2009-01-01             11.0                  0.0 2009-01-01 04:51:00\n",
      "            HourlyWindSpeed  HourlyPrecipitation\n",
      "DATE                                            \n",
      "2009-01-01        11.041667                  0.0\n",
      "2009-01-02         6.083333                  0.0\n",
      "2009-01-03         9.875000                  0.0\n",
      "2009-01-04         7.416667                  0.0\n",
      "2009-01-05         7.000000                  0.0\n"
     ]
    }
   ],
   "source": [
    "hourly_weather_data, daily_weather_data = load_and_clean_weather_data(weather_path1)\n",
    "print(hourly_weather_data.head())\n",
    "print(daily_weather_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "48216557",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATE</th>\n",
       "      <th>HourlyWindSpeed</th>\n",
       "      <th>HourlyPrecipitation</th>\n",
       "      <th>ndate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009-01-01</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2009-01-01 00:51:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2009-01-01</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2009-01-01 01:51:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2009-01-01</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2009-01-01 02:51:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2009-01-01</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2009-01-01 03:51:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2009-01-01</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2009-01-01 04:51:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         DATE  HourlyWindSpeed  HourlyPrecipitation               ndate\n",
       "0  2009-01-01             18.0                  0.0 2009-01-01 00:51:00\n",
       "1  2009-01-01             18.0                  0.0 2009-01-01 01:51:00\n",
       "2  2009-01-01             18.0                  0.0 2009-01-01 02:51:00\n",
       "3  2009-01-01              8.0                  0.0 2009-01-01 03:51:00\n",
       "4  2009-01-01             11.0                  0.0 2009-01-01 04:51:00"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hourly_weather_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4cb386ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HourlyWindSpeed</th>\n",
       "      <th>HourlyPrecipitation</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DATE</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2009-01-01</th>\n",
       "      <td>11.041667</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-01-02</th>\n",
       "      <td>6.083333</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-01-03</th>\n",
       "      <td>9.875000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-01-04</th>\n",
       "      <td>7.416667</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-01-05</th>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            HourlyWindSpeed  HourlyPrecipitation\n",
       "DATE                                            \n",
       "2009-01-01        11.041667                  0.0\n",
       "2009-01-02         6.083333                  0.0\n",
       "2009-01-03         9.875000                  0.0\n",
       "2009-01-04         7.416667                  0.0\n",
       "2009-01-05         7.000000                  0.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_weather_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd101f11",
   "metadata": {},
   "source": [
    "## Part 2: Storing Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f3529cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlalchemy\n",
    "engine = sqlalchemy.create_engine('sqlite:///projects.db', echo=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c122964f",
   "metadata": {},
   "source": [
    "### Add Data to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b51d191c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def write_yellow(df):\n",
    "    #df=get_add_yellowtaxi()\n",
    "    df.to_sql('taxitips',con=engine,if_exists=\"append\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0e68a363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_userb(df=None):\n",
    "    if df==None:\n",
    "        df=get_uberdata(uber_path)\n",
    "    df.to_sql('ubertips',con=engine,if_exists=\"append\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d1ddcdc0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def write_hourlyweather(df1):\n",
    "    # df1,df2=load_and_clean_weather_data(weather_path1)\n",
    "\n",
    "    df1.to_sql('hourlyweather',con=engine,if_exists=\"append\")\n",
    "\n",
    "def write_daylyweather(df2):\n",
    "    # df1,df2=load_and_clean_weather_data(weather_path1)\n",
    "    df2.to_sql('daylyweather',con=engine,if_exists=\"append\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "74004f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2015-01.parquet', 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2015-02.parquet', 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2015-03.parquet', 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2015-04.parquet', 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2015-05.parquet', 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2015-06.parquet', 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2015-07.parquet', 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2015-08.parquet', 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2015-09.parquet', 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2015-10.parquet', 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2015-11.parquet', 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2015-12.parquet', 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2014-01.parquet', 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2014-02.parquet', 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2014-03.parquet', 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2014-04.parquet', 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2014-05.parquet', 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2014-06.parquet', 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2014-07.parquet', 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2014-08.parquet', 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2014-09.parquet', 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2014-10.parquet', 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2014-11.parquet', 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2014-12.parquet', 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2013-01.parquet', 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2013-02.parquet', 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2013-03.parquet', 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2013-04.parquet', 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2013-05.parquet', 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2013-06.parquet', 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2013-07.parquet', 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2013-08.parquet', 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2013-09.parquet', 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2013-10.parquet', 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2013-11.parquet', 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2013-12.parquet', 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2012-01.parquet', 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2012-02.parquet', 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2012-03.parquet', 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2012-04.parquet', 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2012-05.parquet', 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2012-06.parquet', 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2012-07.parquet', 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2012-08.parquet', 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2012-09.parquet', 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2012-10.parquet', 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2012-11.parquet', 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2012-12.parquet', 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2011-01.parquet', 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2011-02.parquet', 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2011-03.parquet', 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2011-04.parquet', 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2011-05.parquet', 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2011-06.parquet', 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2011-07.parquet', 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2011-08.parquet', 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2011-09.parquet', 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2011-10.parquet', 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2011-11.parquet', 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2011-12.parquet', 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2010-01.parquet', 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2010-02.parquet', 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2010-03.parquet', 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2010-04.parquet', 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2010-05.parquet', 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2010-06.parquet', 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2010-07.parquet', 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2010-08.parquet', 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2010-09.parquet', 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2010-10.parquet', 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2010-11.parquet', 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2010-12.parquet', 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2009-01.parquet', 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2009-02.parquet', 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2009-03.parquet', 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2009-04.parquet', 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2009-05.parquet', 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2009-06.parquet', 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2009-07.parquet', 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2009-08.parquet', 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2009-09.parquet', 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2009-10.parquet', 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2009-11.parquet', 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2009-12.parquet']\n",
      "/Users/guowenlu/Desktop/TFA/data/queriesyellow_tripdata_2015-01.parquet\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/guowenlu/Desktop/TFA/data/queriesyellow_tripdata_2015-01.parquet'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-2eef2080d16f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwrite_yellow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_add_yellowtaxi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mwrite_userb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mwrite_weather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-f6eef20e78a7>\u001b[0m in \u001b[0;36mget_add_yellowtaxi\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;31m#df.append(get_yellowtaxi_10(filename))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mwrite_yellow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_yellowtaxi_11\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0;31m#df.append(get_yellowtaxi_11(filename))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;31m# pd.concat([df,get_yellowtaxi_11(filename)],axis=0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-6f0dbb1acaf9>\u001b[0m in \u001b[0;36mget_yellowtaxi_11\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#clean and filter yellow taxi data from 2011\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_yellowtaxi_11\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_parquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pyarrow'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;31m# df=df.sample(n=3000,random_state=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     new_df = df.drop(['tpep_dropoff_datetime', 'RatecodeID','VendorID',\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, use_nullable_dtypes, **kwargs)\u001b[0m\n\u001b[1;32m    457\u001b[0m     \"\"\"\n\u001b[1;32m    458\u001b[0m     \u001b[0mimpl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m     return impl.read(\n\u001b[0m\u001b[1;32m    460\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_nullable_dtypes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_nullable_dtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m     )\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, path, columns, use_nullable_dtypes, storage_options, **kwargs)\u001b[0m\n\u001b[1;32m    212\u001b[0m                 )\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m         path_or_handle, handles, kwargs[\"filesystem\"] = _get_path_or_handle(\n\u001b[0m\u001b[1;32m    215\u001b[0m             \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"filesystem\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36m_get_path_or_handle\u001b[0;34m(path, fs, storage_options, mode, is_dir)\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;31m# fsspec resources can also point to directories\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;31m# this branch is used for example when reading from non-fsspec URLs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0mhandles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m         \u001b[0mfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mpath_or_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    649\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    652\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/guowenlu/Desktop/TFA/data/queriesyellow_tripdata_2015-01.parquet'"
     ]
    }
   ],
   "source": [
    "write_yellow(get_add_yellowtaxi())\n",
    "write_userb()\n",
    "write_weather()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248258e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
